# EER-Research-Project
A research project investigating a new machine learning procedure for optimal variable and model selection.
This proposed method is called Estimated Exhaustive Regression, it was originally proposed in a working paper by noted econometrician
Dr. Antony Davies, most well known for his invention of the first known framework for analyzing multi-dimensional panel data in his doctoral dissertation.
The aforementioned working paper is included in this repository in the pdf called "(draft version) An Exploration of Regression-Based Data Mining Techniques by Antony Davies (2008)."

This novel feature selection algorithm involves an adjustment on the All Subsets Regression aka Best Subset Selection procedure whereby only a random subset of j of all of the possible 2^k - 1 regression specifications evaluated by the traditional version of ASR, and evaluating which of them is best by a Relative Cross-Model Chi-Square Criterion as an alternative to any of the typical regression model performance metrics. This is used in order to better identify which optimal variables selected for inclusion in the final regression specification out of all candidate variables is only selected by the algorithm spuriously.

I am collaborating with Antony Davies on this updating of his original research (the results of which we intend to re-submit for publication in 2023) which
mostly involves the addition of two extra Benchmark Methods to compare EER's performance with, namely: LASSO Regression and Forward Selection Stepwise Regression. 
The original WP only compared EER to Backward Elimination Stepwise Regression and a different computationally feasible version of the Best Subset Selection which only selects regressor candidates from subests of equal size set by the analyst just like his EER procedure, but instead of evaluating which candidates to select via a relative cross-model chi-square critereon, it used k-hold cross-validation. Backward Elimination was kept as the 3rd major Benchmark.

Dr. Davies is running his EER Procedure on 260,000 randomly generated synthetic datasets using Stata, while I am running LASSO Regressions, Backward, and Forward Stepwise Regressions on the same set of 260k datasets using R and comparing the results of EER with them subsequently. The scripts I used to run these benchmark comparisons are called: BE and FS Stepwise Regressions.R, LASSO Regressions.R, LASSO using Lars.R, and LASSO using the 'glmnet' package.R. 

I ended up having to run 2 different attempted replications of my LASSO results in terms of which variables they selected and finding out that not only were neither of them the same as the first one (LASSO Regressions.R which fits each LASSO using an the enet() function from the elastic net package in R), neither were exactly the same as each other either before I decided to investigate further and found out that different fitting functions for LASSO in R from different packages actually pick the lambda value in different ways due to different underlying stopping procedures, so I kept the results of all three of these in! This means we have 5 benchmark comparison methods instead of just 3.

It is important for anyone interested in understanding the findings in this research as to how well his EER algorithm performed compared to the 3, 4, or 5 Benchmarks I ran depending on how you want to count them, to understand how the 260k datasets were created. They were created using the Macro Enabled Excel Workbook in this Repo called real data generator.xlsm. Each of the datasets has 503 rows by 31 columns. The first column contains the synthetic observations on the Dependant Variable called Y starting in the 4th row and going all the way down to the 503rd row for 500 random observations on Y in each dataset. Meanwhile, columns 2-31 all contain 500 random observations in the same rows on candidate regressors whose labels are similarly in the 3rd row, their labels are X1, X2, X3, ..., X28, X29, X30. The first row has a proper row label, cell A1 says Regressor, and from then on, cells B1:AE1 contain binary indicators as to whether that candidate regressor is actually included in what econometricians call the Structural Equation which accurately models the phenomenon or proceess the data is taken as describing. 
The second row is just a copy of the third row except that there is no Y in the second row, so cell A2 is always blank, and the Xs have are not included in cells B2:AE2, so they merely say 1, 2, 3, 4, ..., 29, 30.

The 'top 50' folder contains the first 50 csv file formatted datasets and the 'last 50' folder contains the last 50 of the 260k. Each dataset name is of the general form n1-N2-N3-N4.csv; where n1 is one of the following 4 levels of multicollinearity between the set all true regressors (structural variables/factors) for that dataset: 0, 0.25, 0.5, 0.7; N2 is the number of structural variables in that dataset (this is known with certainty for each synthetic dataset by construction which is the reason to create them via Monte Carlo methods in order to use them to explore the properties of new algorithms or estimators and/or compare the performances of such things against standard Benchmarks as I am doing here for this project) which can be any one of 13 different possible values from 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15; N3 is the Error Variance which is one of the 10 following monotonically increasing (integer) values: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10; and lastly, N4, which Dr. Davies decided to set to 500 for this project, is the number of different synthetic datasets with all three of the aforementioned character traits fixed to generate randomly. Just to make that last point a little clearer, 4 * 13 * 10 * 500 = 260,000 datasets.   
