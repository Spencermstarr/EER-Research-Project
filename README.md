# EER-Research-Project
A research project investigating a new machine learning procedure for optimal variable and model selection.
This proposed method is called Estimated Exhaustive Regression, it was originally proposed in a working paper by noted econometrician
Dr. Antony Davies, most well known for his invention of the first known framework for analyzing multi-dimensional panel data in his doctoral dissertation.
The aforementioned working paper is included in this repository in the pdf called "(draft version) An Exploration of Regression-Based Data Mining Techniques by Antony Davies (2008)."

This novel feature selection algorithm involves an adjustment on the All Subsets Regression aka Best Subset Selection procedure whereby only a random subset of j of all of the possible 2^k - 1 regression specifications evaluated by the traditional version of ASR, and evaluating which of them is best by a Relative Cross-Model Chi-Square Criterion as an alternative to any of the typical regression model performance metrics. This is used in order to better identify which optimal variables selected for inclusion in the final regression specification out of all candidate variables is only selected by the algorithm spuriously.

I am collaborating with Antony Davies on this updating of his original research (the results of which we intend to re-submit for publication in 2023) which
mostly involves the addition of two extra Benchmark Methods to compare EER's performance with, namely: LASSO Regression and Forward Selection Stepwise Regression. 
The original WP only compared EER to Backward Elimination Stepwise Regression and a different computationally feasible version of the Best Subset Selection which only selects regressor candidates from subests of equal size set by the analyst just like his EER procedure, but instead of evaluating which candidates to select via a relative cross-model chi-square critereon, it used k-fold cross-validation. Backward Elimination Stepwise Regression was kept as the 3rd major Benchmark, .but the order in which they will appear will be 1st LASSO, 2nd Backward Elimination Stepwise, and 3rd Forward Selection Stepewise. This was my suggestion because LASSO is more modern and widely used.

Dr. Davies is running his EER Procedure on 260,000 randomly generated synthetic datasets using Gauss, while I am running LASSO Regressions, and the Backward & Forward Stepwise Regressions on the same set of 260k datasets using R with the same random seeds used for each, then calculating the classification performance metrics for them in terms of how well they do at selecting the true structural variables in each synthetic dataset. These structural factors are known explicitly for each of the datsets by construction. The classification performance metrics I calculate for each of the three benchmark variable selection techniques ran are their Accuracy, F1 Score, Positive Predictive Value aka Precision, True Positive Rate aka sensitivity, True Negative Rate aka specificity, False Positive Rate, and False Negative Rate, these are calculated for each of the 260k sets of candidate regressors selected. 
The scripts I used to run these benchmark comparisons all come in two forms, one which can be ran just from the file folder containing all 260k datasets or the way I ran them which is incrementally using file folders with subsets of 5,000, 10,000, or 15,000 datasets in them at a time, and another one which can be ran after the datasets have already been loaded, transformed and pre-processed which can be done using R script for loading & prepping the datasets.R. The final/current versions of these scripts are all in the Stage 2 Scripts folder in this Repository, they are called: BE and FS Stepwise Regressions.R & BE & FS code, but just the parts for the regressions.R, Replicating BE & FS using MASS.R & Replicating BE & FS using MASS, but just the code for running the Regressions.R, LASSO Regressions.R & LASSO code, but just the regression part.R, LASSO using Lars.R & LASSO using Lars (regression part only).R, and finally, LASSO using the 'glmnet' package.R, glmnet LASSO (Regressions only).R, & glmnet with s = 'lambda.lse' LASSO (Regressions only).R.

I ended up having to run 2 different attempted replications of my LASSO results in terms of which variables they selected and finding out that not only were neither of them the same as the first one (LASSO Regressions.R which fits each LASSO using an the enet() function from the elastic net package in R), neither were exactly the same as each other either before I decided to investigate further and found out that different fitting functions for LASSO in R from different packages actually pick the lambda value in different ways due to different underlying stopping procedures, so I kept the results of all three of these in! This means we have 5 benchmark comparison methods instead of just 3. Correction, the lasso results via lars and enet are the same, I must have made a mistake the first time I ran both. Similarly, the results of re-running both directions of Stepwise Regression using the mass package in R provides the same sets of selected variables.

The reason there are 3 different scripts for glment is because originally I accidentally hardcoded the lambda penalty factor for each glmnet lasso beforehand because I didn't realize that the s argument used when selecting coefficients in glmnet is the shrinkage/lambda penalty itself because I wrote the glmnet script after running the lassos using enet and lars first and just sorta altered it and played with it until it ran. So then I re-wrote it selecting the optimal penalty factor using cross-validation and the 1 standard error criterion, but then discovered that it actually performs more poorly than when I had hardcoded an arbitrary lambda of 0.1 for all of them initially by accident. I doubt this will make it into the published version, even in an appendix, but I want to avoid the filedrawer effect in research here, so I included it in the Repo.

It is important for anyone interested in understanding the findings in this research as to how well his EER algorithm performed compared to the 3, 4, or 5 Benchmarks I ran depending on how you want to count them, to understand how the 260k datasets were created. They were created using the Macro Enabled Excel Workbook in this Repo called real data generator.xlsm. Each of the datasets has 503 rows by 31 columns. The first column contains the synthetic observations on the Dependant Variable called Y starting in the 4th row and going all the way down to the 503rd row for 500 random observations on Y in each dataset. Meanwhile, columns 2-31 all contain 500 random observations in the same rows on candidate regressors whose labels are similarly in the 3rd row, their labels are X1, X2, X3, ..., X28, X29, X30. The first row has a proper row label, cell A1 says Regressor, and from then on, cells B1:AE1 contain binary indicators as to whether that candidate regressor is actually included in what econometricians call the Structural Equation which accurately models the phenomenon or proceess the data is taken as describing. 
The second row is just a copy of the third row except that there is no Y in the second row, so cell A2 is always blank, and the Xs have are not included in cells B2:AE2, so they merely say 1, 2, 3, 4, ..., 29, 30.

The 'top 50' folder contains the first 50 csv file formatted datasets and the 'last 50' folder contains the last 50 of the 260k. Each dataset name is of the general form n1-N2-N3-N4.csv; where n1 is one of the following 4 levels of multicollinearity between the set all true regressors (structural variables/factors) for that dataset: 0, 0.25, 0.5, 0.7; N2 is the number of structural variables in that dataset (this is known with certainty for each synthetic dataset by construction which is the reason to create them via Monte Carlo methods in order to use them to explore the properties of new algorithms or estimators and/or compare the performances of such things against standard Benchmarks as I am doing here for this project) which can be any one of 13 different possible values from 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15; N3 is the Error Variance which is one of the 10 following monotonically increasing (integer) values: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10; and lastly, N4, which Dr. Davies decided to set to 500 for this project, is the number of different synthetic datasets with all three of the aforementioned character traits fixed to generate randomly. Just to make that last point a little clearer, 4 * 13 * 10 * 500 = 260,000 datasets.   
